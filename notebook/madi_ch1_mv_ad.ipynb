{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfaYk2Yv4r6Q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Chapter 1. Multivariate Multimodel Anomaly Detection with Negative Sampling\n",
    "\n",
    "Colab to accompany [Interpretable, Multidimensional, Multimodal Anomaly Detection\n",
    "with Negative Sampling for Detection of Device Failure](https://proceedings.icml.cc/static/paper_files/icml/2020/2557-Paper.pdf), Sipple, 2020.\n",
    "\n",
    "In this library we propose a scalable, unsupervised approach to detecting anomalies in the Internet of Things (IoT). Complex devices are connected daily and eagerly generate vast streams of multidimensional measurements characterizing their momentary state.  \n",
    "\n",
    "Occasionally, some devices fail, resulting in a system outage, and postmortem analysis reveals that these devices generated unusual states before the outage. Had technical staff been promptly alerted, they could have preemptively fixed the device. It is often impractical or impossible to predict failures on fixed rules or supervised machine learning methods, because failures are too complex, devices are too new to adequately characterize both normal and failure modes in a specific environment, or the environment changes and puts the device into an unpredictable condition. \n",
    "\n",
    "Examples of such complex networked devices include power and climate control in commercial buildings, servers and computers in data centers, badge readers and alarms in physical security systems, and electromechanical components in powerplants. In this paper, we explore an approach that automatically observes a complex system, generates a normal, multidimensional baseline, and detects anomalous measurements from the incoming data stream.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "This is the initial colab that covers Anomaly Detection in multidimensional data in a series of various topics in Anomaly Detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpJsL1rp4-EL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Quickstart\n",
    "Simply execute the following cell that performs a pip-install of madi from the github repo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ElkHp0BG4aA9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/google/madi.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W10mLDrvmUu7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Detecting and Interpreting Anomalies\n",
    "Consider a target process that generates a sequential stream of multidimensional data points $x(1), x(2),...$, where the $i^{th}$ data point $x(i)$ , is a $D$-dimensional vector $x=\\left \\{x_{1}, x_{2},...,x_{D}\\right \\}$ in $\\mathbb{R}^{D}$. The target process may be either a single discrete unit, or a homogeneous cohort of equivalent units. Our objectives are to (a) estimate $P(x\\in Normal)$, and (b) attribute the anomaly score on to each dimension as an interpretation for each anomalous $x$.\n",
    "\n",
    "\n",
    "**Definition 1.** An *anomaly* is any data point $x$ with a near zero probability that it was generated by the Normal process: $P(x\\in Normal)\\approx 0$.\n",
    "\n",
    "The **Normal** process occupies one or more discrete manifolds or volumes of unknown shape, in $\\mathbb{R}^{D}$, which are populated in high density by the target process, except for the small subset of anomalous points that fall outside the Normal volume(s). The **Anomalous** volume is the complement of the Normal volume in $\\mathbb{R}^{D}$. A **unimodal process** occupies a single discrete volume, and a **multimodal process** occupies two or more spatially disconnected volumes.\n",
    "\n",
    "###Detecting Anomalies with Negative Sampling\n",
    "The Concentration Phenomenon states that there is exponentially more room in higher dimensions than in lower dimensions. Intuitively, as the number of dimensions $D$ increases, the cube has $2^{D}$ corners, so most of the volume is concentrated near them, and manifolds tend to occupy less volume relative to the full space (vanhandel2016, vershynin2018).  Based on the Concentration Phenomenon, we propose a simple method for developing a labeled data set to train a classifier for our anomaly detection task.\n",
    "\n",
    "Our approach to anomaly detection is to define two class samples, and train a classifier function $F:\\mathbb{R}^{D}\\rightarrow [0,1]$ to distinguish between the two classes. The **positive class sample** $U=\\left \\{ u(1), u(2),...,u(M) \\right \\}$ is the set of $M$ $D$-dimensional data points\\footnote{Every $u$ and $v$ are $D$-dimensional points like $x$, and we use letters $u$ and $v$ to distinguish the positive (observed) sample $U$ from the negative sample $V$.} that were observed by the target process, including a small number of unlabeled, actual anomalies, which our anomaly detection algorithm is expected to detect. Because the positive sample is contaminated with a small number of anomalous data points, there is also a false positive labeling error, $P(u\\in Anomalous)>0$. However, because anomalies are rare by definition, the probability that any point drawn from the positive sample is normal is nearly one, independent of dimensionality $D$.  The **negative class sample** $V=\\left \\{ v(1), v(2),...,v(N) \\right \\}$  is chosen independently and uniformly from the cube bounded by the extrema of each dimension plus some small $\\pm\\delta$, where the volume bounded by the negative sample is strictly greater than the volume bounded by the positive sample, $Vol(U)<Vol(V)$. The **sample ratio**, $r_{s}=\\frac{N}{M}$, governs the ratio between the negative sample size and the positive sample size, where $r_{s}=0$ represents the one-class anomaly detection classifier. Since the negative class is intended to represent the space of anomalies, and there is a nonzero probability that a negative sample point will land within the normal region, $P(v\\in Normal)>0$, we also have a false negative labeling error. \n",
    "\n",
    "**Assumption 1** (Sufficiency): *The positive sample U is representative of a stationary, ergodic Normal process.* As with all supervised approaches, the training set should reflect the prediction set, and it is essential to sample enough to reflect all Normal modes of behavior. In monitoring climate control devices, it is important to sample from all hours of the day, and days of the week, even when fixing seasonal conditions. \n",
    "\n",
    "Ideally, we would like to train a binary classifier $F$ with data that has as few labeling errors as possible. Intuitively, it makes sense to develop an algorithm that carefully selects the negative sample to avoid the Normal space. For example, Gonzalez2002 proposes a type of region-growing approach that avoids choosing points close to the positive sample.  However, such a sampling approach is difficult and/or computationally expensive in high dimensions because we are not able to characterize the positive volume. Instead, we observe that volumes tend to contract in high-dimensional spaces, and propose using uniform i.i.d. sampling for generating the negative sample.\n",
    "\n",
    "\n",
    "**Proposition 1.** (Uniform Negative Sampling): For each dimension $d \\leq D$, let $lim_{d}=\\left [ min\\left ( U_{d} \\right ) -\\delta, max\\left ( U_{d} \\right ) +\\delta  \\right ]$ be a range bounded by the extrema of the positive sample $U$ extended by a conservative positive length $\\delta$ that extends $lim_{d}$ beyond the normal space. We assume that the sample size of $U$ is sufficiently large to bound the Normal region. Choose a negative sample $V$, by selecting $N$ points uniformly i.i.d. bounded by $lim_{d}$ for each $d \\leq D$. In high dimension, $ D \\rightarrow \\infty $, false negative sampling error decays exponentially to zero, regardless of the shape of the Normal region.}\n",
    "\n",
    "*Proof.* While it can be shown that specific shapes such as the sphere or Gaussian contract to zero volume in high dimensions, we choose the hypercube for the normal volume with lengths bounded by the extrema $\\Delta u_{d}=max \\left ( U_{d} \\right ) - min \\left ( U_{d} \\right )$ since its relative volume decreases most slowly with increasing dimension. Since we are sampling uniformly, the probability of a false positive is the relative volume $P\\left ( v \\in Normal \\right ) = \\frac{Vol(U)}{Vol(V)}$.  The length of dimension $d$ in the negative volume is $\\Delta v_{d} = \\Delta u_{d} + 2 \\delta$ . Since $\\Delta u_{d} < \\Delta v_{d}$ for all $d \\leq D$, as the dimensionality $D$ increases, the false negative error descends exponentially to zero $P \\left ( v \\in Normal \\right ) = \\lim_{ D  \\rightarrow \\infty } \\prod_{d}^{D}\\frac{\\Delta v_{d}}{\\Delta u_{d} }=0$ .    \n",
    "\n",
    "The rate at which $P \\left ( v \\in Normal \\right )$ decreases depends on the geometry and volume of the Normal region. Proposition 1 provides upper-bounded asymptotic guarantees that can be strengthened given knowledge of the geometry of the Normal volume. However, when the characteristics of the Normal region are unknown, without loss of generality, we can use Proposition 1 to bound the false negative probability $P \\left ( v \\in Normal \\right ) \\leq \\prod_{d}^{D}\\frac{\\Delta v_{d}}{\\Delta u_{d} }$ Next, we apply Proposition 1 to develop a simple procedure for developing a dataset that can be used to train an anomaly detection classifier.\n",
    "\n",
    "\n",
    "**Proposition 2.** (Labeled Training Set for Anomaly Detection): *Given a sufficiently sampled, high-dimensional dataset from a target process and uniform negative sampling, we can generate a labeled two-class dataset to train a classifier $F$ for detecting anomalies.*\n",
    "\n",
    "A good training set requires a low number of labeling errors for a classifier to learn decision boundaries. In this application, both false positive and false negative errors are small. By our definition of an anomaly, false negative occurrences are rare and $P \\left ( u \\in Anomalous \\right ) \\approx 0$, so $U$ can be used in training data to represent the Normal class. By Proposition 1, uniform negative sampling can be used to generate accurate labeled data representing the anomalous regions, when $ D  \\rightarrow \\infty$.  \n",
    "\n",
    "The **sampling ratio**, $r_{s}$, specifies the density of negative data points since the Negative Sample volume is fixed. In lower dimensions, it is possible to oversample in the Negative Sample, such that $P \\left ( v \\in Normal \\right ) \\approx P \\left ( u \\in Normal \\right )$ and a classifier is unable to learn decision boundaries. The sample ratio should be chosen within $r_{s,min}:P \\left ( u \\in Anomalous \\right ) \\ll P \\left ( v \\in Anomalous \\right )$ and $r_{s,max}:P \\left ( v \\in Normal \\right ) \\ll P \\left ( u \\in Normal \\right )$.\n",
    "\n",
    "\n",
    "Given that the classifiers are universal function approximators, such as deep ReLU networks (Hanin2019), there is no limitation to the number of distinct modes, shapes, or orientations of continuous, high-dimensional Normal volumes. Negative-sampling classifiers detect “bad interactions”, where the values of all dimensions are within Normal ranges, but in aggregate, the points are in the Anomalous region.  \n",
    "\n",
    "If the positive sample size, the dimensionality, and the classifier hyperparameters (number of estimators, tree depth, number layers, layer width, etc.) are fixed, and we assume that, in general, classifier training time grows linearly with the size of input, then the run-time complexity incurred during training of negative sampling depends on the sampling ratio only, $O(r_{s})$, and remains constant, $O(1)$, at inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "khx2J8FdP5ug",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Colab Imports\n",
    "import sys\n",
    "import time\n",
    "import madi\n",
    "from madi.utils import file_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from madi.datasets import gaussian_mixture_dataset\n",
    "from madi.detectors.neg_sample_neural_net_detector import NegativeSamplingNeuralNetworkAD\n",
    "from madi.detectors.isolation_forest_detector import IsolationForestAd\n",
    "from madi.detectors.integrated_gradients_interpreter import IntegratedGradientsInterpreter\n",
    "from madi.detectors.one_class_svm import OneClassSVMAd\n",
    "from madi.detectors.neg_sample_random_forest import NegativeSamplingRandomForestAd\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.version.VERSION > '2.1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6nMYx4XZCsO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#1. Select a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "tv3SZUdnu3Fg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Choose the data set\n",
    "_RESOURCE_LOCATION = \"madi.datasets.data\"\n",
    "data_source = \"gaussian_mixture\" #@param [\"gaussian_mixture\", \"smart_buildings\"]\n",
    "ds = None\n",
    "\n",
    "class InvalidDatasetError(ValueError):\n",
    "    pass\n",
    "\n",
    "if data_source == 'gaussian_mixture':\n",
    "  contamination = 0.15\n",
    "\n",
    "  ds = gaussian_mixture_dataset.GaussianMixtureDataset(\n",
    "          n_dim=16,\n",
    "          n_modes=2,\n",
    "          n_pts_pos=8000,\n",
    "          sample_ratio=contamination,\n",
    "          upper_bound=3,\n",
    "          lower_bound=-3)\n",
    "  \n",
    "  print('Loaded Gaussian mixture with 2 modes in 16 dimensions, with a sample size of %d.' %len(ds.sample))\n",
    "\n",
    "elif data_source == 'smart_buildings':\n",
    "  \n",
    "  data_file = file_utils.PackageResource(\n",
    "      _RESOURCE_LOCATION, \"anomaly_detection_sample_1577622599.csv\")\n",
    "  readme_file = file_utils.PackageResource(\n",
    "      _RESOURCE_LOCATION, \"anomaly_detection_sample_1577622599_README.md\")\n",
    "  ds = madi.datasets.smart_buildings_dataset.SmartBuildingsDataset(data_file, readme_file)\n",
    "  print(ds.description)\n",
    "\n",
    "else:\n",
    "  raise InvalidDatasetError(\"You requested an invalid data set (%s).\" %data_source)\n",
    "\n",
    "\n",
    "print('Randomize the data, and split into training and test sample.')\n",
    "split_ix = int(len(ds.sample) * 0.8)\n",
    "training_sample = ds.sample.iloc[:split_ix]\n",
    "test_sample = ds.sample.iloc[split_ix:]\n",
    "print(\"\\tTraining sample size: %d\" %len(training_sample))\n",
    "print(\"\\tTest sample size: %d\" %len(test_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_MRIhR96JLF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#2. Configure Anomaly Detectors\n",
    "\n",
    "\n",
    "**Negative Sampling Neural Net**\n",
    "\n",
    "The NS-NN is based on a very basic neural network of a few stackes (usually 1 - 3) of dense (with ReLU activations) and dropout hidden layers. In this architecture, each hidden layer as the same number of nodes. The input layer  has width $D$, and the output layer is a sigmoid. We apply a binary cross-entropy loss and use RMS Prop as the optimizer. \n",
    "\n",
    "The training data is a permuted sample of positive (with `class_label`  = 1) and negative data points (with `class_label` = 0). \n",
    "\n",
    "*  `sample_ratio`: The size ratio between negative and positive samples, e.g., `sample_ratio` = 2 means that there are twice as many negative sample points as positive.\n",
    "*  `sample_delta`: The extension behond the range define by the min and max of the positive sample for each dimension, e.g., `sample_delta`  = 0.5 means that the negative samples in a range between 2.5% beyond positive sample min and max.\n",
    "*  `batch_size`: Training batch size for neural net classifier.\n",
    "* `steps_per_epoch`: Number of steps per training epoch for neural network.\n",
    "* `epochs`: total number of epochs in training.\n",
    "* `dropout`: The dropout fraction for each dropout layer.\n",
    "* `layer_width`: number of nodes at each layer.\n",
    "* `n_hidden_layers`: number of hidden layers between input layer and output sigmoid layer.\n",
    "\n",
    "**Negative Sampling Neural Net**\n",
    "\n",
    "NS-NN and NS-RF use the same type of negative sampling to build a classifier training set.\n",
    "*  `sample_ratio`: The size ratio between negative and positive samples, e.g., `sample_ratio` = 2 means that there are twice as many negative sample points as positive.\n",
    "*  `sample_delta`: The extension behond the range define by the min and max of the positive sample for each dimension, e.g., `sample_delta`  = 0.5 means that the negative samples in a range between 2.5% beyond positive sample min and max.\n",
    "*  Additional parameters for Random Forest Classifiers are avilable at [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "**Isolation Forest**\n",
    "\n",
    "Please see [sklearn.ensemble.IsolationForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) for detailed explanation of the parameters.\n",
    "\n",
    "**One-Class SVM**\n",
    "\n",
    "Please see [sklearn.svm.OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html) for a detailed explanantion of the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "mOGOqYTIZM_W",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Reset Anomlay Detectors\n",
    "ad_dict = {}\n",
    "log_dir = \"logs/nsnn2\" #@param {type:\"string\"}\n",
    "\n",
    "# Set up the logging directory.\n",
    "!mkdir -p $log_dir\n",
    "\n",
    "# Isolation Forest Parameters\n",
    "iso_params = {}\n",
    "\n",
    "# One-class SVM Parameters\n",
    "ocsvm_params = {}\n",
    "\n",
    "# Neg Sampling Random Forest Parameters\n",
    "nsrf_params = {}\n",
    "\n",
    "# Neg Sampling Neural Net Parameters\n",
    "nsnn_params = {}\n",
    "\n",
    "\n",
    "if data_source == 'gaussian_mixture':\n",
    "\n",
    "    iso_params['n_estimators'] = 40\n",
    "    iso_params['max_samples'] = 0.1\n",
    "    iso_params['contamination'] = 0.05\n",
    "    iso_params['max_features'] = 0.44\n",
    "\n",
    "    ocsvm_params['kernel'] = 'rbf'\n",
    "    ocsvm_params['shrinking'] = True\n",
    "    ocsvm_params['nu'] = 0.04  # Contamination\n",
    "    ocsvm_params['gamma'] = 'scale'\n",
    "    ocsvm_params['coef0'] = 0.0\n",
    "\n",
    "    nsrf_params['sample_ratio']=10.0\n",
    "    nsrf_params['sample_delta']=0.05\n",
    "    nsrf_params['num_estimators'] = 150\n",
    "    nsrf_params['criterion'] = 'gini'\n",
    "    nsrf_params['max_depth'] = 50\n",
    "    nsrf_params['min_samples_split'] = 12\n",
    "    nsrf_params['min_samples_leaf'] = 5\n",
    "    nsrf_params['min_weight_fraction_leaf'] = 0.06\n",
    "    nsrf_params['max_features'] = 0.3\n",
    "    nsrf_params['sample_ratio'] = 2.0\n",
    "    nsrf_params['sample_delta'] = 0.05\n",
    "\n",
    "    nsnn_params['sample_ratio']=10.0\n",
    "    nsnn_params['sample_delta']=0.05\n",
    "    nsnn_params['batch_size']=16\n",
    "    nsnn_params['steps_per_epoch']=80\n",
    "    nsnn_params['epochs']=180\n",
    "    nsnn_params['dropout']=0.7\n",
    "    nsnn_params['layer_width']=145\n",
    "    nsnn_params['n_hidden_layers']=3\n",
    "\n",
    "elif data_source == 'smart_buildings':\n",
    "\n",
    "    iso_params['n_estimators'] = 100\n",
    "    iso_params['max_samples'] = 0.01\n",
    "    iso_params['contamination'] = 0.27\n",
    "    iso_params['max_features'] = 0.7\n",
    "\n",
    "    ocsvm_params['kernel'] = 'rbf'\n",
    "    ocsvm_params['shrinking'] = True\n",
    "    ocsvm_params['nu'] = 0.36  # Contamination\n",
    "    ocsvm_params['gamma'] = 'scale'\n",
    "    ocsvm_params['coef0'] = 0.0\n",
    "\n",
    "    nsrf_params['sample_ratio']=21.00\n",
    "    nsrf_params['sample_delta']=0.05\n",
    "    nsrf_params['num_estimators'] = 150\n",
    "    nsrf_params['criterion'] = 'gini'\n",
    "    nsrf_params['max_depth'] = 50\n",
    "    nsrf_params['min_samples_split'] = 10\n",
    "    nsrf_params['min_samples_leaf'] = 5\n",
    "    nsrf_params['min_weight_fraction_leaf'] = 0.06\n",
    "    nsrf_params['max_features'] = 0.26\n",
    "\n",
    "    nsnn_params['sample_ratio']=25.0\n",
    "    nsnn_params['sample_delta']=0.05\n",
    "    nsnn_params['batch_size']=32\n",
    "    nsnn_params['steps_per_epoch']=16\n",
    "    nsnn_params['epochs']=100\n",
    "    nsnn_params['dropout']=0.85\n",
    "    nsnn_params['layer_width']=150\n",
    "    nsnn_params['n_hidden_layers']=2\n",
    "\n",
    "else:\n",
    "  raise InvalidDatasetError(\"You requested an invalid data set (%s).\" %data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "maEdoLd_oGSv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Add in Isolation Forest Anomaly Detector (iso)\n",
    "ad_dict['iso'] =IsolationForestAd(n_estimators=iso_params['n_estimators'], \n",
    "                               max_samples=iso_params['max_samples'], \n",
    "                               contamination=iso_params['contamination'], \n",
    "                               max_features=iso_params['max_features'], \n",
    "                               bootstrap=False, n_jobs=None, behaviour='new', \n",
    "                               random_state=None, verbose=0)\n",
    "\n",
    "print('Anomaly Detectors: ', list(ad_dict))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "9leLnlWugOOf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Add in One-Class SVM (oc-svm)\n",
    "ad_dict['oc-svm'] = OneClassSVMAd(\n",
    "    kernel=ocsvm_params['kernel'],\n",
    "    degree=3,\n",
    "    gamma=ocsvm_params['gamma'],\n",
    "    coef0=ocsvm_params['coef0'],\n",
    "    tol=0.001,\n",
    "    nu=ocsvm_params['nu'],\n",
    "    shrinking=ocsvm_params['shrinking'],\n",
    "    cache_size=200,\n",
    "    verbose=False,\n",
    "    max_iter=-1)\n",
    "\n",
    "print('Anomaly Detectors: ', list(ad_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "gpE630UMhGMr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Add in Negative Sampling Random Forest (ns-rf)\n",
    "ad_dict['ns-rf']= NegativeSamplingRandomForestAd(\n",
    "        n_estimators=nsrf_params['num_estimators'],\n",
    "        criterion=nsrf_params['criterion'],\n",
    "        max_depth=nsrf_params['max_depth'],\n",
    "        min_samples_split=nsrf_params['min_samples_split'],\n",
    "        min_samples_leaf=nsrf_params['min_samples_leaf'],\n",
    "        min_weight_fraction_leaf=nsrf_params['min_weight_fraction_leaf'],\n",
    "        max_features=nsrf_params['max_features'],\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        min_impurity_split=None,\n",
    "        bootstrap=True,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        class_weight=None,\n",
    "        sample_delta=nsrf_params['sample_delta'],\n",
    "        sample_ratio=nsrf_params['sample_ratio'])\n",
    "\n",
    "print('Anomaly Detectors: ', list(ad_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "g13c757EchOb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Add in Negative Sampling Neural Net (ns-nn)\n",
    "ad_dict['ns-nn'] = NegativeSamplingNeuralNetworkAD(\n",
    "        sample_ratio=nsnn_params['sample_ratio'],\n",
    "        sample_delta=nsnn_params['sample_delta'],\n",
    "        batch_size=nsnn_params['batch_size'],\n",
    "        steps_per_epoch=nsnn_params['steps_per_epoch'],\n",
    "        epochs=nsnn_params['epochs'],\n",
    "        dropout=nsnn_params['dropout'],\n",
    "        layer_width=nsnn_params['layer_width'],\n",
    "        n_hidden_layers=nsnn_params['n_hidden_layers'],\n",
    "        log_dir=log_dir)\n",
    "\n",
    "print('Anomaly Detectors: ', list(ad_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nhHH6lfAlPzx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Execute Cross-Fold Validation {output-height:\"unlimited\"}\n",
    "number_crossfolds =  1#@param {type:\"integer\"}\n",
    "number_folds =  5#@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "def fold_sample(sample: pd.DataFrame, n_folds: int = 5) ->  List[Dict[str, pd.DataFrame]]:\n",
    "  \"\"\"Splits a sample into N folds.\n",
    "  \n",
    "  Args:\n",
    "    sample: training/test sample to be folded.\n",
    "  \"\"\"\n",
    "  sample = shuffle(sample)\n",
    "  \n",
    "  folds = []\n",
    "  # Split into train and test folds, and assign to list called folds.\n",
    "  for training_sample_idx, test_sample_idx in KFold(n_splits=5).split(sample):\n",
    "    test_sample = sample.iloc[test_sample_idx]\n",
    "    training_sample = sample.iloc[training_sample_idx]\n",
    "    folds.append({\"train\": training_sample, \"test\": test_sample})\n",
    "  return folds\n",
    "\n",
    "\n",
    "def plot_auc(ad_results: Dict[str, Dict[str, Dict[str, np.array]]], \n",
    "             experiment_name: str):\n",
    "  \"\"\"Plots the ROC AUC. \"\"\"\n",
    "  \n",
    "  fig, ax = plt.subplots(figsize=(15, 15))\n",
    "  start = 0.0\n",
    "  stop = 1.0\n",
    "  colors = [cm.jet(x) for x in np.linspace(start, stop, len(ad_results))]\n",
    "\n",
    "  df_auc = pd.DataFrame()\n",
    "\n",
    "  lw = 2\n",
    "  ix = 0\n",
    "  for ad_id in ad_results:\n",
    "\n",
    "    fold_results = ad_results[ad_id]\n",
    "    vfprs = []\n",
    "    vtprs = []\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 200)\n",
    "    validation_aucs = []\n",
    "    for fold_id in fold_results:\n",
    "      fpr = fold_results[fold_id]['fpr']\n",
    "      tpr = fold_results[fold_id]['tpr']\n",
    "\n",
    "      validation_auc_val = auc(fpr, tpr)\n",
    "      validation_aucs.append(validation_auc_val)    \n",
    "      interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "      interp_tpr[0] = 0.0\n",
    "      tprs.append(interp_tpr)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "  \n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = np.mean(validation_aucs)\n",
    "    df_auc[ad_id] = [mean_auc]\n",
    "\n",
    "    plt.plot(mean_fpr, mean_tpr, color=colors[ix], lw=lw, \n",
    "             label='%s: %0.1f%% (%d)' % (\n",
    "                 ad_id, 100.0 * mean_auc, len(fold_results) ))\n",
    "    ix+=1\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    \n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    \n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='gray', alpha=0.5,\n",
    "                label=None)\n",
    "    \n",
    "  ax.grid(linestyle='-', linewidth='0.5', color='darkgray')\n",
    "  plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "  plt.xlim([0.0, 1.05])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('AUC curves for %s' %experiment_name)\n",
    "  \n",
    "  legend = plt.legend(loc='lower right', shadow=False, fontsize='20')\n",
    "  legend.get_frame().set_facecolor('white')\n",
    "\n",
    "  for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(20)\n",
    "  \n",
    "  for sp in ax.spines:\n",
    "    ax.spines[sp].set_color(\"black\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "anomaly_detectors = sorted(list(ad_dict))\n",
    "experiment_name = \"%s with %s\" %(ds.name, \", \".join(anomaly_detectors))\n",
    "\n",
    "df_results = pd.DataFrame(columns = ['ad', 'auc', 'extime'])\n",
    "ad_results = {}\n",
    "\n",
    "for ad in anomaly_detectors:\n",
    "\n",
    "  if ad not in ad_results:\n",
    "    ad_results[ad] = {}\n",
    "\n",
    "  for cx_run in range(number_crossfolds):\n",
    "    folds = fold_sample(ds.sample, n_folds = number_folds)\n",
    "\n",
    "    for fid in range(number_folds):\n",
    "      \n",
    "      fold = folds[fid]\n",
    "\n",
    "      # Drop the class label from the training sample, since this is unsupervised.\n",
    "      training_sample = fold['train'].copy()\n",
    "      testing_sample = fold['test'].copy()\n",
    "      X_train = training_sample.drop(columns = ['class_label'])\n",
    "      X_test  = testing_sample.drop(columns = ['class_label'])\n",
    "      y_test  = testing_sample['class_label']\n",
    "\n",
    "      start_time = time.time()\n",
    "\n",
    "      # Train a model in the training split.\n",
    "      ad_dict[ad].train_model(x_train=X_train)\n",
    "\n",
    "      # Predict on the test set.\n",
    "      y_predicted = ad_dict[ad].predict(X_test)['class_prob']\n",
    "\n",
    "      # Compute the AUC on the test set. \n",
    "      auc_value = madi.utils.evaluation_utils.compute_auc(\n",
    "              y_actual=y_test, y_predicted=y_predicted)\n",
    "        \n",
    "      # Compute the ROC curve.\n",
    "      fpr, tpr, _ = roc_curve(y_test, y_predicted)\n",
    "\n",
    "      end_time = time.time()\n",
    "      extime = end_time - start_time\n",
    "      ad_results[ad]['%03d-%02d' %(cx_run, fid)] = {'fpr': fpr, 'tpr':tpr}\n",
    "      df_results.loc['%03d-%02d-%s' %(cx_run, fid, ad)] = [ad, auc_value, extime]\n",
    "\n",
    "      # Refresh the output area.\n",
    "      clear_output()\n",
    "      \n",
    "      plot_auc(ad_results, experiment_name  = experiment_name)\n",
    "      \n",
    "      del training_sample\n",
    "      del testing_sample\n",
    "print(\"Final Results:\")\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SdiYu4y9IEc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Next steps\n",
    "In the next chapter, we’ll explore variable attribution using Integrated Gradients. This provides some interpretability by blaming the dimensions that have the greatest effect on the anomaly score."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch 1 AD with Neg Sampling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}